{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8U396s7MBI"
      },
      "source": [
        "### Install on google colab lab Axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtFwPORX1alD",
        "outputId": "e61ac166-27d6-443b-aa5c-1954801a3cf9"
      },
      "outputs": [],
      "source": [
        "!git clone -q https://github.com/OpenAccess-AI-Collective/axolotl\n",
        "%cd axolotl\n",
        "!pip install -qqq packaging huggingface_hub --progress-bar off\n",
        "!pip install -qqq -e '.[flash-attn,deepspeed]' --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qd8v-UZ7Q4X"
      },
      "source": [
        "### Hugging face login\n",
        "\n",
        "\n",
        "Hugging Face API token: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGbOOxtA4Lzs",
        "outputId": "41e52c19-30d7-4771-a26b-a9e8f8586d02"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJCCk9G67ui7"
      },
      "source": [
        "### Configuration file\n",
        "\n",
        "The finetuning technique configuration below is set to use QLoRA, but you can change it. See example axolotl configurations for different llms and finetuning techniques [here](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy8Dq9sJ1aw1"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "user_name = \"abdelrahman-alkhodary\"\n",
        "fine_tuned_model_name = \"EvolCodeLlama-7b-qlora\"\n",
        "new_model = f\"{user_name}/{fine_tuned_model_name}\"\n",
        "\n",
        "# The base model that will be fine-tuned\n",
        "base_model = \"codellama/CodeLlama-7b-hf\"\n",
        "# The dataset used to fine-tune the base model\n",
        "dataset_path=\"mlabonne/Evol-Instruct-Python-1k\" \n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: {base_model}\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: LlamaTokenizer\n",
        "is_llama_derived_model: true\n",
        "hub_model_id: {hub_model_id}\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: {dataset_path}\n",
        "    type: alpaca\n",
        "dataset_prepared_path:\n",
        "val_set_size: 0.05\n",
        "output_dir: ./qlora-out\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: axolotl\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 1\n",
        "micro_batch_size: 10\n",
        "num_epochs: 3\n",
        "optimizer: paged_adamw_32bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "warmup_steps: 100\n",
        "eval_steps: 0.01\n",
        "save_strategy: epoch\n",
        "save_steps:\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "    bos_token: \"<s>\"\n",
        "    eos_token: \"</s>\"\n",
        "    unk_token: \"<unk>\"\n",
        "\"\"\"\n",
        "\n",
        "yaml_string = yaml_string.format(\n",
        "  base_model=base_model, \n",
        "  dataset_path=dataset_path,\n",
        "  hub_model_id=new_model\n",
        ")\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "yaml_file = 'config.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(yaml_file, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgWKrRuyEZ9i"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "b501xQLf18Fp",
        "outputId": "48ae94ff-e92d-499a-81f6-2b82aed0d98b"
      },
      "outputs": [],
      "source": [
        "!axolotl train config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMqTOFlLEUI1"
      },
      "source": [
        "### Merge the model and adapters\n",
        "The QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also merge the base  language model with this adapter and push the merged model to Hugging Face Hub by following the next two steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VgaSXe9GEdel",
        "outputId": "6d442fcb-0429-49ca-f606-6546b43cef27"
      },
      "outputs": [],
      "source": [
        "!axolotl merge-lora config.yaml --lora-model-dir=\"./completed-model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTziow2FEeAI"
      },
      "source": [
        "### 6. Upload the merged model to hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4ckfNdnFDXuW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "import getpass\n",
        "\n",
        "# Prompt the user to enter the Hugging Face token securely\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face API token: \")\n",
        "\n",
        "# Save the token for the current session\n",
        "HfFolder.save_token(hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zg796crAAl0o",
        "outputId": "b06ac3c8-f602-467b-f8a8-3fe8b7fc6b4e"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "# HF_TOKEN defined in the secrets tab in Google Colab\n",
        "api = HfApi()\n",
        "\n",
        "# Upload merge folder\n",
        "api.create_repo(\n",
        "    repo_id=new_model,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=new_model,\n",
        "    folder_path=\"qlora-out/merged\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
