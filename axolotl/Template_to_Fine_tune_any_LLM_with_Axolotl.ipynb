{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8U396s7MBI"
      },
      "source": [
        "### Install on google colab lab Axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtFwPORX1alD",
        "outputId": "e61ac166-27d6-443b-aa5c-1954801a3cf9"
      },
      "outputs": [],
      "source": [
        "!pip install --no-build-isolation axolotl[deepspeed]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qd8v-UZ7Q4X"
      },
      "source": [
        "### Hugging face login\n",
        "\n",
        "\n",
        "Hugging Face API token: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGbOOxtA4Lzs",
        "outputId": "41e52c19-30d7-4771-a26b-a9e8f8586d02"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJCCk9G67ui7"
      },
      "source": [
        "### Configuration file\n",
        "\n",
        "The finetuning technique configuration below is set to use QLoRA, but you can change it. See example axolotl configurations for different llms and finetuning techniques [here](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy8Dq9sJ1aw1"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "user_name = \"abdelrahman-alkhodary\"\n",
        "fine_tuned_model_name = \"Meta-Llama-3.1-8B-qlora\"\n",
        "new_model = f\"{user_name}/{fine_tuned_model_name}\"\n",
        "\n",
        "# The base model that will be fine-tuned\n",
        "base_model = \"NousResearch/Meta-Llama-3.1-8B\"\n",
        "# The dataset used to fine-tune the base model\n",
        "dataset_path=\"mlabonne/Evol-Instruct-Python-1k\" \n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: {base_model}\n",
        "hub_model_id: {hub_model_id}\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: tatsu-lab/alpaca\n",
        "    type: alpaca\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.05\n",
        "output_dir: ./outputs/lora-out\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "eval_sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_modules_to_save:\n",
        "  - embed_tokens\n",
        "  - lm_head\n",
        "\n",
        "wandb_project:\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 1\n",
        "num_epochs: 1\n",
        "optimizer: paged_adamw_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 2e-5\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: false\n",
        "sdp_attention: true\n",
        "\n",
        "warmup_steps: 1\n",
        "max_steps: 25\n",
        "evals_per_epoch: 1\n",
        "eval_table_size:\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|end_of_text|>\n",
        "\"\"\"\n",
        "\n",
        "yaml_string = yaml_string.format(\n",
        "  base_model=base_model, \n",
        "  hub_model_id=new_model\n",
        ")\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "yaml_file = 'config.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(yaml_file, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgWKrRuyEZ9i"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "b501xQLf18Fp",
        "outputId": "48ae94ff-e92d-499a-81f6-2b82aed0d98b"
      },
      "outputs": [],
      "source": [
        "!accelerate launch -m axolotl.cli.train /content/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VgaSXe9GEdel",
        "outputId": "6d442fcb-0429-49ca-f606-6546b43cef27"
      },
      "outputs": [],
      "source": [
        "!accelerate launch -m axolotl.cli.inference /content/config.yaml --lora_model_dir=\"./outputs/lora-out\" --gradio"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
